import streamlit as st
import torch
import torch.nn as nn
import cv2
import numpy as np
from PIL import Image
from torchvision import transforms
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, WebRtcMode
import av

# Import class model c·ªßa b·∫°n
from model import FaceEmotionCNN 

# 1. C·∫•u h√¨nh & Load Model
CLASSES = ['angry', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'suprise']
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_PATH = 'final_model.pth'

# Load Face Detection (Haar Cascade c√≥ s·∫µn trong cv2)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

@st.cache_resource
def load_model():
    model = FaceEmotionCNN(num_classes=len(CLASSES), in_channels=1)
    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    except FileNotFoundError:
        st.error(f"Kh√¥ng t√¨m th·∫•y file {MODEL_PATH}. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ ƒë·ªÉ file model c√πng th∆∞ m·ª•c.")
        return None
    model.to(DEVICE)
    model.eval()
    return model

# Load model m·ªôt l·∫ßn duy nh·∫•t
emotion_model = load_model()

# ƒê·ªãnh nghƒ©a Transform gi·ªëng l√∫c train (Grayscale -> Resize -> Tensor)
val_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((75, 75)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# 2. Class x·ª≠ l√Ω video Real-time
class EmotionProcessor(VideoTransformerBase):
    def recv(self, frame):
        # Chuy·ªÉn frame t·ª´ ƒë·ªãnh d·∫°ng av sang numpy array (OpenCV)
        img = frame.to_ndarray(format="bgr24")

        # 1. Chuy·ªÉn sang ·∫£nh x√°m ƒë·ªÉ t√¨m khu√¥n m·∫∑t
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # 2. Ph√°t hi·ªán khu√¥n m·∫∑t
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        for (x, y, w, h) in faces:
            # 3. C·∫Øt v√πng khu√¥n m·∫∑t (ROI - Region of Interest)
            roi_gray = gray[y:y+h, x:x+w]
            
            # X·ª≠ l√Ω ngo·∫°i l·ªá n·∫øu m·∫∑t qu√° nh·ªè
            if roi_gray.size == 0:
                continue

            # 4. Ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ ƒë∆∞a v√†o model (d√πng transform ƒë√£ ƒë·ªãnh nghƒ©a)
            try:
                roi_tensor = val_transform(roi_gray).unsqueeze(0).to(DEVICE)

                # 5. D·ª± ƒëo√°n
                with torch.no_grad():
                    outputs = emotion_model(roi_tensor)
                    probs = torch.nn.functional.softmax(outputs, dim=1)[0]
                    conf, pred_idx = torch.max(probs, 0)
                    
                emotion_label = CLASSES[pred_idx.item()]
                confidence = conf.item() * 100

                # 6. V·∫Ω khung v√† nh√£n l√™n h√¨nh g·ªëc
                color = (0, 255, 0) # M√†u xanh l√°
                if emotion_label in ['angry', 'fear', 'disgust', 'sad']:
                    color = (0, 0, 255) # M√†u ƒë·ªè cho c·∫£m x√∫c ti√™u c·ª±c
                
                cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)
                cv2.putText(img, f"{emotion_label} ({confidence:.1f}%)", (x, y-10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
            except Exception as e:
                pass # B·ªè qua l·ªói x·ª≠ l√Ω frame ƒë·ªÉ stream kh√¥ng b·ªã ng·∫Øt

        # Tr·∫£ v·ªÅ frame ƒë√£ v·∫Ω h√¨nh
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# 3. Giao di·ªán Streamlit
st.title("üé• Real-time Emotion Recognition")
st.write("S·ª≠ d·ª•ng Webcam ƒë·ªÉ nh·∫≠n di·ªán c·∫£m x√∫c theo th·ªùi gian th·ª±c.")

# Th√™m tu·ª≥ ch·ªçn (Sidebar)
app_mode = st.sidebar.selectbox("Ch·ªçn ch·∫ø ƒë·ªô", ["Webcam Realtime", "Upload ·∫¢nh"])

if app_mode == "Webcam Realtime":
    if emotion_model is not None:
        ctx = webrtc_streamer(
            key="example", 
            mode=WebRtcMode.SENDRECV, # Quan tr·ªçng: Ch·∫ø ƒë·ªô g·ª≠i v√† nh·∫≠n
            video_processor_factory=EmotionProcessor,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": False}
        )
        st.info("Nh·∫•n 'START' ƒë·ªÉ b·∫≠t Camera. H√£y ƒë·∫£m b·∫£o ƒë·ªß √°nh s√°ng ƒë·ªÉ nh·∫≠n di·ªán t·ªët nh·∫•t.")

elif app_mode == "Upload ·∫¢nh":
    # (Gi·ªØ l·∫°i code upload ·∫£nh c≈© c·ªßa b·∫°n ·ªü ƒë√¢y n·∫øu mu·ªën)
    uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh...", type=["jpg", "png", "jpeg"])
    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert('RGB')
        st.image(image, caption='·∫¢nh g·ªëc', width=300)
        
        # Code x·ª≠ l√Ω ·∫£nh tƒ©nh (c·∫ßn th√™m ph·∫ßn detect face cho ·∫£nh tƒ©nh ƒë·ªÉ ch√≠nh x√°c h∆°n)
        # ·ªû ƒë√¢y demo ƒë∆°n gi·∫£n ƒë∆∞a c·∫£ ·∫£nh v√†o (nh∆∞ code c≈©) ho·∫∑c b·∫°n c√≥ th·ªÉ update logic detect face v√†o ƒë√¢y.
        if st.button('D·ª± ƒëo√°n'):
            st.title("üé≠ Facial Emotion Recognition")
            st.write("Upload m·ªôt b·ª©c ·∫£nh khu√¥n m·∫∑t ƒë·ªÉ AI ƒëo√°n c·∫£m x√∫c nh√©!")

            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh...", type=["jpg", "png", "jpeg"],key="upload_image_1")

            if uploaded_file is not None:
                # Hi·ªÉn th·ªã ·∫£nh g·ªëc
                image = Image.open(uploaded_file)
                st.image(image, caption='·∫¢nh ƒë√£ upload', width=300)
                
                if st.button('D·ª± ƒëo√°n c·∫£m x√∫c'):
                    with st.spinner('ƒêang ph√¢n t√≠ch...'):
                        try:
                            # Load model v√† x·ª≠ l√Ω ·∫£nh
                            model = load_model()
                            img_tensor = process_image(image)
                            
                            # D·ª± ƒëo√°n
                            with torch.no_grad():
                                outputs = model(img_tensor)
                                # D√πng Softmax ƒë·ªÉ ra x√°c su·∫•t %
                                probs = torch.nn.functional.softmax(outputs, dim=1)[0]
                            
                            # L·∫•y k·∫øt qu·∫£ cao nh·∫•t
                            conf, pred_idx = torch.max(probs, 0)
                            pred_label = CLASSES[pred_idx.item()]
                            
                            # Hi·ªÉn th·ªã k·∫øt qu·∫£
                            st.success(f"D·ª± ƒëo√°n: **{pred_label.upper()}** ({conf.item()*100:.2f}%)")
                            
                            # V·∫Ω bi·ªÉu ƒë·ªì x√°c su·∫•t c√°c l·ªõp kh√°c
                            st.write("---")
                            st.write("Chi ti·∫øt x√°c su·∫•t:")
                            probs_dict = {class_name: float(prob) for class_name, prob in zip(CLASSES, probs)}
                            st.bar_chart(probs_dict)
                            
                        except Exception as e:
                            st.error(f"C√≥ l·ªói x·∫£y ra: {e}")