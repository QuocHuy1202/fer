import streamlit as st
import torch
import torch.nn as nn
import cv2
import numpy as np
from PIL import Image
from torchvision import transforms
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, WebRtcMode, RTCConfiguration
import av

# Import class model c·ªßa b·∫°n
from model import FaceEmotionCNN 

# -----------------------------------------------------------
# 1. C·∫§U H√åNH & LOAD MODEL
# -----------------------------------------------------------
CLASSES = ['angry', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'suprise']
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_PATH = 'final_model.pth'

# C·∫•u h√¨nh STUN Server cho WebRTC (ƒë·ªÉ ch·∫°y online)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# Load Face Detection (Haar Cascade)
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

@st.cache_resource
def load_model():
    model = FaceEmotionCNN(num_classes=len(CLASSES), in_channels=1)
    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    except FileNotFoundError:
        st.error(f"Kh√¥ng t√¨m th·∫•y file {MODEL_PATH}. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ ƒë·ªÉ file model c√πng th∆∞ m·ª•c.")
        return None
    model.to(DEVICE)
    model.eval()
    return model

emotion_model = load_model()

# Transform cho model (Grayscale -> 75x75 -> Tensor)
val_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((75, 75)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# -----------------------------------------------------------
# 2. X·ª¨ L√ù WEBCAM REALTIME
# -----------------------------------------------------------
class EmotionProcessor(VideoTransformerBase):
    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        for (x, y, w, h) in faces:
            roi_gray = gray[y:y+h, x:x+w]
            if roi_gray.size == 0: continue

            try:
                roi_tensor = val_transform(roi_gray).unsqueeze(0).to(DEVICE)
                with torch.no_grad():
                    outputs = emotion_model(roi_tensor)
                    probs = torch.nn.functional.softmax(outputs, dim=1)[0]
                    conf, pred_idx = torch.max(probs, 0)
                
                emotion_label = CLASSES[pred_idx.item()]
                confidence = conf.item() * 100

                # V·∫Ω khung
                color = (0, 255, 0)
                if emotion_label in ['angry', 'fear', 'disgust', 'sad']:
                    color = (0, 0, 255)
                
                cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)
                cv2.putText(img, f"{emotion_label} ({confidence:.1f}%)", (x, y-10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
            except Exception:
                pass
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# -----------------------------------------------------------
# 3. GIAO DI·ªÜN CH√çNH
# -----------------------------------------------------------
st.title("üé• AI Emotion Recognition")
app_mode = st.sidebar.selectbox("Ch·ªçn ch·∫ø ƒë·ªô", ["Webcam Realtime", "Upload ·∫¢nh"])

if app_mode == "Webcam Realtime":
    st.write("S·ª≠ d·ª•ng Webcam ƒë·ªÉ nh·∫≠n di·ªán c·∫£m x√∫c theo th·ªùi gian th·ª±c.")
    if emotion_model is not None:
        ctx = webrtc_streamer(
            key="realtime-emotion", 
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=EmotionProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True
        )
        st.info("Nh·∫•n 'START' v√† cho ph√©p tr√¨nh duy·ªát truy c·∫≠p Camera.")

elif app_mode == "Upload ·∫¢nh":
    st.write("Upload ·∫£nh ch·ª©a khu√¥n m·∫∑t ƒë·ªÉ nh·∫≠n di·ªán.")
    uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh...", type=["jpg", "png", "jpeg"])

    if uploaded_file is not None:
        # Load ·∫£nh v√† hi·ªÉn th·ªã
        image_pil = Image.open(uploaded_file).convert('RGB')
        st.image(image_pil, caption='·∫¢nh g·ªëc', width=400)
        
        if st.button('üîç Ph√¢n t√≠ch c·∫£m x√∫c'):
            with st.spinner('ƒêang t√¨m khu√¥n m·∫∑t v√† ph√¢n t√≠ch...'):
                # Chuy·ªÉn ƒë·ªïi sang format OpenCV ƒë·ªÉ t√¨m m·∫∑t
                img_cv = np.array(image_pil) 
                img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR) # Convert RGB to BGR
                gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
                
                # T√¨m khu√¥n m·∫∑t
                faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                
                if len(faces) == 0:
                    st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t n√†o trong ·∫£nh! ƒêang th·ª≠ ph√¢n t√≠ch to√†n b·ªô ·∫£nh...")
                    # N·∫øu kh√¥ng th·∫•y m·∫∑t, th·ª≠ ƒë∆∞a c·∫£ ·∫£nh v√†o (resize v·ªÅ 75x75)
                    face_roi = gray 
                    display_img = img_cv
                else:
                    st.success(f"ƒê√£ t√¨m th·∫•y {len(faces)} khu√¥n m·∫∑t.")
                    # L·∫•y khu√¥n m·∫∑t to nh·∫•t (ho·∫∑c ƒë·∫ßu ti√™n) ƒë·ªÉ x·ª≠ l√Ω
                    (x, y, w, h) = faces[0]
                    face_roi = gray[y:y+h, x:x+w]
                    
                    # V·∫Ω khung l√™n ·∫£nh ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£
                    display_img = img_cv.copy()
                    cv2.rectangle(display_img, (x, y), (x+w, y+h), (0, 255, 0), 3)

                # D·ª± ƒëo√°n
                try:
                    roi_tensor = val_transform(face_roi).unsqueeze(0).to(DEVICE)
                    
                    with torch.no_grad():
                        outputs = emotion_model(roi_tensor)
                        probs = torch.nn.functional.softmax(outputs, dim=1)[0]
                    
                    # K·∫øt qu·∫£
                    conf, pred_idx = torch.max(probs, 0)
                    pred_label = CLASSES[pred_idx.item()]
                    
                    # Hi·ªÉn th·ªã
                    col1, col2 = st.columns(2)
                    with col1:
                        st.image(cv2.cvtColor(display_img, cv2.COLOR_BGR2RGB), caption="V·ªã tr√≠ khu√¥n m·∫∑t", width=300)
                    with col2:
                        st.metric(label="C·∫£m x√∫c d·ª± ƒëo√°n", value=pred_label.upper())
                        st.progress(int(conf.item() * 100))
                        st.write(f"ƒê·ªô tin c·∫≠y: **{conf.item()*100:.2f}%**")
                    
                    # Bi·ªÉu ƒë·ªì chi ti·∫øt
                    st.write("---")
                    probs_dict = {name: float(p) for name, p in zip(CLASSES, probs)}
                    st.bar_chart(probs_dict)
                    
                except Exception as e:
                    st.error(f"L·ªói khi x·ª≠ l√Ω: {e}")